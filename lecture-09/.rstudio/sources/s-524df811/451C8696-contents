Classification
========================================================
author: Vincent Toups
date: 11 Sept 2020
width:1400
height:800
css:style.css


```{r echo=FALSE, include=FALSE}
library(tidyverse);
library(gridExtra);

last <- function(s){
    s[[length(s)]];
}

first <- function(s){
    s[[1]]
}

block <- function(bod){
    eval(substitute(function()b,c(b=quote(bod))))()
}

save_plotly_in <- function(dir_name, file_name, p){
    withr::with_dir(dir_name, htmlwidgets::saveWidget(plotly::ggplotly(p), file_name));
}

```

```{r echo=FALSE}
block({
    d <- tibble(id=seq(1000),
            x=c(rnorm(500,-3,2),
            rnorm(500,3,2)), 
            classification=c(rep(1,500),
                             rep(2,500)));
            
    p <- ggplot(d) + geom_point(aes(x,y=runif(1000),color=factor(classification)));
    p
});
```

Clustering with some example data.

Clustering and Classification
========================================================

1. Closely related Machine Learning Techniques
2. Classification: assign new data to one of a known category 
   given by some labeled data.
3. Clustering: assign data to one of a set of clusters determined
   in some way from the data itself.
   
Classification is the easier problem by far. You want classification problems.
Clustering problems are a stygian pit.

![](./images/mnist.png)

With enough data, and assuming the labels are good, you can build a classifier
for anything.

Vocabulary
========================================================

1. Training Data - build the model
2. Validation Data - tune the parameters
3. Testing Data - evaluate its "true" performance

```{r echo=FALSE, messages=FALSE, warnings=FALSE}
block({
    rects <- tibble(xmin=c(0,0.7,0.85),
                    xmax=c(0.7,0.85,1),
                    Dataset=factor(c("Train","Validate","Test")));
    rects$ymin <- 0;
    rects$ymax <- 1;
    r <- ggplot(rects) +
        geom_rect(aes(xmin=xmin,ymin=ymin,ymax=ymax,xmax=xmax,fill=Dataset)) +
        labs(x="proportion of dataset");
    grid.arrange(r,nrow=4);
});
```

Toy Data
========

```{r }
d <- tibble(x=c(rnorm(500,-3,2),
                rnorm(500,3,2)), 
            classification=factor(c(rep("C1",500),
                                    rep("C2",500))),
            set=factor(c(rep("Train",700),
                         rep("Validate",150),
                         rep("Test",150))
                       %>%
                       sample(1000,replace=FALSE))) %>%
    mutate(id=seq(1000));
d
```

Toy Data
========

```{r echo=FALSE}
ggplot(d,aes(x)) + geom_density(aes(fill=classification),alpha=0.4);
```

Simplest Possible Classification
================================

K-Nearest Neighbor.

Find the K-nearest neighbors in the training data. Each such neighbor
votes for whatever its class is. Normalizing these votes produces a
"probability" for each classification - the largest is the best guess.

Code
====

```{r}
library(class);
train <- d %>% filter(set=="Train");
validate <- d %>% filter(set=="Validate");
test <- d %>% filter(set=="Test");

tr <- matrix(train$x, nrow=nrow(train));
tt <- matrix(validate$x, nrow=nrow(validate));

results <- do.call(rbind, Map(function(n){
    predictions <- knn(train = tr,
                       test = tt,
                       cl = train$classification,
                       k = n);
    acc <- sum(predictions == validate$classification)/length(predictions);
    tibble(k=n,acc=acc);   
}, seq(1,100))) %>% arrange(desc(acc),k);
```

Results
=======

```{r echo=FALSE}
ggplot(results,aes(k,acc)) + geom_line() + labs(x="k",y="accuracy");
```

Best?
=====

In higher dimensions finding N neighbors can be tricky if done naively so we
may prefer the model with the smallest number of neighbors (within reason).

```{r}
results %>% arrange(desc(acc),k) %>% head(3);
```

Test Set Accuracy
=================

```{r}
pred <- knn(train = tr,
            test = matrix(test$x, nrow=nrow(test)),
            cl = train$classification,
            k = 17);
sum(pred==test$classification)/nrow(test);
```

Class Imbalance
===============

I've seen PhD Statisticians mess this up.

What if your training data isn't balanced?

Is it a sampling error or does it reflect a true fact about the prior
distribution of results?

In some situations this isn't particularly well defined. For instance,
you are training a classifier to distinguish between cats and
dogs. What is the prior probability of a picture being of a cat?

Accuracy Lies
=============

Consider:

```{r }
source("utils.R");

d <- read_csv("./source_data/datasets_38396_60978_charcters_stats.csv") %>%
    tidy_up_names() %>%
    mutate(neutral=alignment == "neutral") %>%
    drop_na();

means <- d %>% group_by(neutral) %>% summarize(p=length(intelligence)/nrow(d),
        int=mean(intelligence),
        str=mean(strength),
        spd=mean(speed),
        dur=mean(durability),
        pwr=mean(power),
        cbt=mean(combat)); means;
```

Simple Models Do Well On Unbalanced Data
========================================

```{r echo=FALSE}
means
```
1. 98% of all characters are not neutral.
2. So a model which always guesses "not neutral" has an accuracy of
   98%.
3. It is always wrong for non-neutral characters.
4. We need better metrics.

But Also Consider
=================

1. True Positive Rate  (tp): how often we predict a neutral correctly. (0 naive)
2. True Negative Rate  (tn): how often we predict a non-neutral correctly (1 naive)
3. False Positive Rate (fp): how often do we predict a non-neutral as neutral. (0 naive)
4. False Negative Rate (fn): how often do we predict neutral as non-neutral (1 naive)

Since these measures are over individual classes we can use them to
concoct summary results which are better at characterizing our system.

F1 Score
========

![](./images/f1.png)
![](./images/prec-recall.png)
![](./images/f1-alt.png)

F1 is the harmonic mean of precision and recall.

F1 Score
========

```{r}
f1 <- MLmetrics::F1_Score;
f1
```

Train Test Validate
===================

This is tricky - we need some data from each category but we don't
have a lot of neutral characters.

Lots of approaches here. One is that we subsample by category and
recombine the data.

```{r}
model_split <- function(dfi, train_p, validate_p, test_p, col_name="exp_group"){
    dfi <- sample_n(dfi, nrow(dfi),replace=FALSE);
    p <- (seq(nrow(dfi))-1)/nrow(dfi);
    train_dfi <- dfi %>% filter(p < train_p);
    validate_dfi <- dfi %>% filter(p < train_p + validate_p & p >= train_p);
    test_dfi <- dfi %>% filter(p >= train_p + validate_p);
    train_dfi[[col_name]] <- "train";
    validate_dfi[[col_name]] <- "validate";
    test_dfi[[col_name]] <- "test";
    rbind(train_dfi, validate_dfi, test_dfi);
}

d <- read_csv("./source_data/faux-powers.csv");

d <- rbind(model_split(d %>% filter(neutral==TRUE), 1/3, 1/3, 1/3),
           model_split(d %>% filter(neutral==FALSE), 1/3, 1/3, 1/3));
d %>% group_by(neutral, exp_group) %>% tally()

```

Generalized Linear Models for Logistic Regression
=================================================

KNN fails for somewhat obvious reasons on this data set. Let's break
out a GLM and do a logistic regression.

```{r}
train <- d %>% filter(exp_group=="train");
validate <- d %>% filter(exp_group=="validate");
test <- d %>% filter(exp_group=="test");
model <- glm(neutral ~ intelligence +
                 strength +
                 speed +
                 durability +
                 power +
                 combat,family=binomial(link='logit'),data=train)
pred <- predict(model, newdata=validate, type="response");
sum(pred>0.5 == validate$neutral)/nrow(validate);
```

Varying the Threshold
=====================

```{r}
ggplot(tibble(p_neutral=pred),aes(p_neutral)) + geom_density();
```

The distribution of probabilities of a data point being "neutral". If
we lower our threshold for what counts as neutral presumably we'll
eventually start getting some right.

ROC Curve
=========

Plot True Positive Rate against False Postive Rate.

```{r}

roc <- do.call(rbind, Map(function(threshold){
    p <- pred > threshold;
    tp <- sum(p[validate$neutral])/sum(validate$neutral);
    fp <- sum(p[!validate$neutral])/sum(!validate$neutral);
    tibble(threshold=threshold,
           tp=tp,
           fp=fp)
},seq(100)/100))

ggplot(roc, aes(fp,tp)) + geom_line() + xlim(0,1) + ylim(0,1) +
    labs(title="ROC Curve",x="False Positive Rate",y="True Positive Rate");

```

Choosing A Threshold
====================

We usually choose a threshold on the basis of some cost-benefit
analysis.

If the cost of a false positive is is negligible, then we want to
lower the theshold. Otherwise we need to find the threshold where the
cost associated with a false positive is balanced by the benefits of 
detecting the condition.

This requires a good estimate of the prior probability.



