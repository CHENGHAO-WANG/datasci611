{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Web Scraping for Fun and Profit?\n",
    "================================\n",
    "\n",
    "Sometimes you need data and the web is the only place to get it.\n",
    "\n",
    "This isn't just for a lark - you might have an official data set to which you might like to join some other data which isn't available a a CSV somewhere but is available in some form on the web.\n",
    "\n",
    "Today we'll cover the basics of web scraping.\n",
    "\n",
    "What is Scraping\n",
    "================\n",
    "\n",
    "Scraping is an ugly name for what is sometimes an ugly process: we will use our programming language to download web pages (maybe) parse them into some kind of Document Object and then we will use queries on that object to extract the data we want.\n",
    "\n",
    "Languages\n",
    "=========\n",
    "\n",
    "You can use almost any language to scrape from the web as long as you can do those two steps:\n",
    "\n",
    "1. issue a request over HTTP (sometimes with some complicated body or header)\n",
    "2. parse the response (often HTML sometimes JSON or some other kind of document)\n",
    "3. interact with the result.\n",
    "\n",
    "Today we will be using a Python Module called beautiful soup.\n",
    "\n",
    "HTTP/REST/HTML refresher\n",
    "========================\n",
    "\n",
    "The web is a bunch of computers listening for HTTP (or HTTPS) connections. HTTP (hypertext transfer protocol) is a standard for inter-computer communication in which a client (us) and a server (them) exchange messages in the form of documents. These documents adhere to a standard about which we don't need to know too much. What we do need to know is that every message contains a\n",
    "\n",
    "1. header - contains what you might call meta-data about the request\n",
    "2. body - contains the document associated with the request\n",
    "\n",
    "The body can be empty (and often is for simple requests). Most responses contain a body.\n",
    "\n",
    "Your browser is a request generating engine. All it does is make HTTP requests and then render or otherwise utilize the response. When we scrape from the web we use a program other than a browser to make the requests and instead of rendering the result we parse it and serach inside of it.\n",
    "\n",
    "Many message responses contain HTML in the body.\n",
    "\n",
    "HTML\n",
    "----\n",
    "\n",
    "HTML stands for \"Hypertext Markup Language.\" You are familiar with one Markup language via this class: (R)Markdown. RMarkdown began as a simple way to represent (eg 'Mark up') a subset of HTML. People used to write HTML by hand but it is verbose:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "\t<title>This is Hello World page</title>\n",
    "</head>\n",
    "<body>\n",
    " \t<h1>Hello World</h1>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "The key idea behind HTML is that the text is \"marked up\" with \"tags\" that, in conjunction with a stylesheet, tell the browser how to render a page. We are going to be pulling data from the page so we don't need to worry so much about the way that the document is rendered. But the tags in an HTML document often tell us a great deal about the structure of the data on the page. We can use the tags to \"monkey bar\" around the document.\n",
    "\n",
    "Here is a list of HTML tags:\n",
    "\n",
    "```\n",
    "div, span, h1, h2, h3, h4, ul, ol, li, a\n",
    "```\n",
    "\n",
    "There are others but this is the most common set. In order to allow more flexibility to web page designers HTML tags often have attributes. These look like:\n",
    "\n",
    "```\n",
    "<div id=\"important-stuff\" class=\"big important\">Some very important text</div>\n",
    "```\n",
    "\n",
    "Attributes can have any names whatsoever but \"id\" and \"class\" are special and very common. The \"id\" tag is a unique identifier for an element and is thus (often) useful for finding a specific piece of information on a page. \"class\" has to do with CSS but also typically picks out a group of similar pieces of content. \n",
    "\n",
    "If you are lucky during scraping your data of interest lives in a specific tag with a known id or in a group of elements with a known class.\n",
    "\n",
    "The first step in any scraping exercise is to look at the source of the page you are interested in scraping from.\n",
    "\n",
    "An Example\n",
    "----------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<!DOCTYPE html>\\n<html>\\n<head><meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\\n\\t<title>Raccoons have a brute cleverness.</title>\\n\\t<link href=\"css/toast.css\" rel=\"stylesheet\" type=\"text/css\" />\\n\\t<link href=\"css/index.css\" rel=\"stylesheet\" type=\"text/css\" />\\n\\t<link href=\"/static/procyonic-favicon.ico\" rel=\"shortcut icon\" />\\n</head>\\n<body>\\n<div class=\"container\">\\n<div class=\"grid\">\\n<div class=\"unit span-grid\">\\n<div class=\"pre-title\">(<a href=\"mailto:vincent.toups@gmail.com\">Vincent Toups</a>)</div>\\n\\n<div class=\"pre-title\">(<a href=\"cv.html\">Curriculum Vitae</a>)</div>\\n\\n<h1 class=\"title\">-Procyonic-</h1>\\n</div>\\n\\n<div class=\"unit span-grid\"><canvas class=\"centered\" id=\"skull\"></canvas></div>\\n\\n<div class=\"unit one-of-three content-column about\">\\n<h1 class=\"sub-title\">-Being-</h1>\\n\\n<figure>\\n<p class=\"centered-image-holder\"><img class=\"for-storage\" id=\"hidden-about-image\" src=\"static/vincent-small.png\" /></p>\\n\\n<figcaption>(art by <a href=\"https://www.facebook.com/girlowar\">Katie Ellison</a>)</figcaption>\\n</figure>\\n\\n<p>I&#39;m a data scientist, software engineer, physicist, game designer/artist, partner and dad. I also have my assistant beekeeper&#39;s merit badge and I sometimes dig holes to plant trees in my partner&#39;s orchard.</p>\\n\\n<p></p>\\n</div>\\n\\n<div class=\"unit one-of-three content-column\">\\n<h1 class=\"sub-title\">-Doing-</h1>\\n\\n<p class=\"centered-image-holder\"><img src=\"static/clock-small.png\" /></p>\\n\\n<p>I am a senior data scientist at the Collaborative Studies Coordinating Center at UNC Chapel Hill. With what little free time I have I work on game development and learning about the fundamental degrees of freedom of the universe and their ontological implications.</p>\\n\\n<h3 class=\"sub-sub-title\">-Projects &amp; Code-</h3>\\n\\n<ul class=\"circle\">\\n\\t<li><a href=\"https://featurecreeps.itch.io/corpsewizard\">The Death Of The Corpse Wizard</a> is a minimalist arena/arcade roguelike written almost 100% in the programming language Scheme.</li>\\n\\t<li><a href=\"/meatIsMulder/meatIsMulder.html\">MEAT IS MULDER</a> is a generative art piece which automatically reconstructs images related to 20th Century Fox&#39;s &quot;The X Files&quot; with fragments of images of meat, dead leaves, and snakes.</li>\\n\\t<li><a href=\"/clocks/index.html\">Clocks</a> is a generative art project (HTML5/Canvas) which tries to make interesting images out of non-random but complex behaviors. The above image was produced by one such clock. The clocks are written in...</li>\\n\\t<li><a href=\"https://github.com/VincentToups/gazelle\">Gazelle</a> is a Lisp-like language targeting Javascript. Gazelle has built-in support for modules, metaprogramming, and extensible pattern matching. Gazelle&#39;s pattern matching is based on...</li>\\n\\t<li><a href=\"https://github.com/VincentToups/shadchen\">Shadchen</a>, the name of which derives from a Yiddish word for matchmaker, is an ml-ish pattern-matching library for Common Lisp. I maintain two versions of this library, one in Common Lisp and one in <a href=\"https://github.com/VincentToups/shadchen-el\">Emacs Lisp</a>. The latter provides special forms which implement self-recursion without growing the stack (most Common Lisp implementations are capable of tail call elimination) and forms for doing computations inside monads (I tend to put experimental code into the Emacs Lisp version first).</li>\\n\\t<li><a href=\"https://github.com/VincentToups/patty\">Patty</a> is a super-tiny pattern matcher for <a href=\"http://picolisp.com/5000/!wiki?home\">Picolisp</a>.</li>\\n\\t<li>For many years I was devoted to finding patterns in neural spike trains produced by grazing bifurcations. While it has been some time since I worked with that code, it is available <a href=\"https://github.com/VincentToups/matlab-utils\">here</a>. Also of interest may be...</li>\\n\\t<li><a href=\"https://github.com/VincentToups/parenlab\">Parenlab</a>, which is sort of like Gazelle for Matlab, a Lisp dialect that targets Matlab and Matlab-like languages while preserving syntactic features for making vector-based programming easier. Parenlab is a bit idiosyncratic, so if you are interested in using it, let me know and I will help you out.</li>\\n\\t<li><a href=\"https://github.com/VincentToups/\">My github</a> hosts all sorts of other fun stuff, including a giant pile of emacs-lisp utilities, a <a href=\"http://dorophone.blogspot.com/2012/02/almost-pure-random-demon-name-generator.html\">Random Demon Name Generator</a> (almost purely functional), some persistent data structures for Emacs Lisp and Common Lisp, a version of Parenscript that will run on Franz Lisp&#39;s Modern Mode and behave about as well as that implies, and a bunch of other cool stuff which is probably not clean enough to talk up on its own.</li>\\n</ul>\\n</div>\\n\\n<div class=\"unit one-of-three content-column\">\\n<h1 class=\"sub-title\">-Saying-</h1>\\n\\n<p class=\"centered-image-holder\"><canvas id=\"speech-canvas\"></canvas></p>\\n\\n<p><a href=\"/blog\">Dorophone</a> (archive <a href=\"http://dorophone.blogspot.com\">here</a>) is my blog. I don&#39;t have nearly as much time to write as I did back in my languorous graduate student days, but I still update it periodically. </p>\\n\\n</div>\\n\\n<div class=\"unit span-grid footer\">\\n<div>This site was created with <a href=\"http://www.gnu.org/software/emacs/\">GNU Emacs</a> and <a href=\"https://daneden.me/toast/\">Toast.css</a> It depends on HTML5 features, so use a newish browser!</div>\\n</div>\\n</div>\\n</div>\\n<script src=\"src/main.js\"></script></body>\\n</html>\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests as r\n",
    "from bs4 import BeautifulSoup\n",
    "r.get(\"https://procyonic.org\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can make requests lets wrap everything up into a handy dandy wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html><head><meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
       "\t<title>Raccoons have a brute cleverness.</title>\n",
       "\t<link href=\"css/toast.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "\t<link href=\"css/index.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "\t<link href=\"/static/procyonic-favicon.ico\" rel=\"shortcut icon\"/>\n",
       "</head>\n",
       "<body>\n",
       "<div class=\"container\">\n",
       "<div class=\"grid\">\n",
       "<div class=\"unit span-grid\">\n",
       "<div class=\"pre-title\">(<a href=\"mailto:vincent.toups@gmail.com\">Vincent Toups</a>)</div>\n",
       "\n",
       "<div class=\"pre-title\">(<a href=\"cv.html\">Curriculum Vitae</a>)</div>\n",
       "\n",
       "<h1 class=\"title\">-Procyonic-</h1>\n",
       "</div>\n",
       "\n",
       "<div class=\"unit span-grid\"><canvas class=\"centered\" id=\"skull\"></canvas></div>\n",
       "\n",
       "<div class=\"unit one-of-three content-column about\">\n",
       "<h1 class=\"sub-title\">-Being-</h1>\n",
       "\n",
       "<figure>\n",
       "<p class=\"centered-image-holder\"><img class=\"for-storage\" id=\"hidden-about-image\" src=\"static/vincent-small.png\"/></p>\n",
       "\n",
       "<figcaption>(art by <a href=\"https://www.facebook.com/girlowar\">Katie Ellison</a>)</figcaption>\n",
       "</figure>\n",
       "\n",
       "<p>I'm a data scientist, software engineer, physicist, game designer/artist, partner and dad. I also have my assistant beekeeper's merit badge and I sometimes dig holes to plant trees in my partner's orchard.</p>\n",
       "\n",
       "<p></p>\n",
       "</div>\n",
       "\n",
       "<div class=\"unit one-of-three content-column\">\n",
       "<h1 class=\"sub-title\">-Doing-</h1>\n",
       "\n",
       "<p class=\"centered-image-holder\"><img src=\"static/clock-small.png\"/></p>\n",
       "\n",
       "<p>I am a senior data scientist at the Collaborative Studies Coordinating Center at UNC Chapel Hill. With what little free time I have I work on game development and learning about the fundamental degrees of freedom of the universe and their ontological implications.</p>\n",
       "\n",
       "<h3 class=\"sub-sub-title\">-Projects &amp; Code-</h3>\n",
       "\n",
       "<ul class=\"circle\">\n",
       "\t<li><a href=\"https://featurecreeps.itch.io/corpsewizard\">The Death Of The Corpse Wizard</a> is a minimalist arena/arcade roguelike written almost 100% in the programming language Scheme.</li>\n",
       "\t<li><a href=\"/meatIsMulder/meatIsMulder.html\">MEAT IS MULDER</a> is a generative art piece which automatically reconstructs images related to 20th Century Fox's \"The X Files\" with fragments of images of meat, dead leaves, and snakes.</li>\n",
       "\t<li><a href=\"/clocks/index.html\">Clocks</a> is a generative art project (HTML5/Canvas) which tries to make interesting images out of non-random but complex behaviors. The above image was produced by one such clock. The clocks are written in...</li>\n",
       "\t<li><a href=\"https://github.com/VincentToups/gazelle\">Gazelle</a> is a Lisp-like language targeting Javascript. Gazelle has built-in support for modules, metaprogramming, and extensible pattern matching. Gazelle's pattern matching is based on...</li>\n",
       "\t<li><a href=\"https://github.com/VincentToups/shadchen\">Shadchen</a>, the name of which derives from a Yiddish word for matchmaker, is an ml-ish pattern-matching library for Common Lisp. I maintain two versions of this library, one in Common Lisp and one in <a href=\"https://github.com/VincentToups/shadchen-el\">Emacs Lisp</a>. The latter provides special forms which implement self-recursion without growing the stack (most Common Lisp implementations are capable of tail call elimination) and forms for doing computations inside monads (I tend to put experimental code into the Emacs Lisp version first).</li>\n",
       "\t<li><a href=\"https://github.com/VincentToups/patty\">Patty</a> is a super-tiny pattern matcher for <a href=\"http://picolisp.com/5000/!wiki?home\">Picolisp</a>.</li>\n",
       "\t<li>For many years I was devoted to finding patterns in neural spike trains produced by grazing bifurcations. While it has been some time since I worked with that code, it is available <a href=\"https://github.com/VincentToups/matlab-utils\">here</a>. Also of interest may be...</li>\n",
       "\t<li><a href=\"https://github.com/VincentToups/parenlab\">Parenlab</a>, which is sort of like Gazelle for Matlab, a Lisp dialect that targets Matlab and Matlab-like languages while preserving syntactic features for making vector-based programming easier. Parenlab is a bit idiosyncratic, so if you are interested in using it, let me know and I will help you out.</li>\n",
       "\t<li><a href=\"https://github.com/VincentToups/\">My github</a> hosts all sorts of other fun stuff, including a giant pile of emacs-lisp utilities, a <a href=\"http://dorophone.blogspot.com/2012/02/almost-pure-random-demon-name-generator.html\">Random Demon Name Generator</a> (almost purely functional), some persistent data structures for Emacs Lisp and Common Lisp, a version of Parenscript that will run on Franz Lisp's Modern Mode and behave about as well as that implies, and a bunch of other cool stuff which is probably not clean enough to talk up on its own.</li>\n",
       "</ul>\n",
       "</div>\n",
       "\n",
       "<div class=\"unit one-of-three content-column\">\n",
       "<h1 class=\"sub-title\">-Saying-</h1>\n",
       "\n",
       "<p class=\"centered-image-holder\"><canvas id=\"speech-canvas\"></canvas></p>\n",
       "\n",
       "<p><a href=\"/blog\">Dorophone</a> (archive <a href=\"http://dorophone.blogspot.com\">here</a>) is my blog. I don't have nearly as much time to write as I did back in my languorous graduate student days, but I still update it periodically. </p>\n",
       "\n",
       "</div>\n",
       "\n",
       "<div class=\"unit span-grid footer\">\n",
       "<div>This site was created with <a href=\"http://www.gnu.org/software/emacs/\">GNU Emacs</a> and <a href=\"https://daneden.me/toast/\">Toast.css</a> It depends on HTML5 features, so use a newish browser!</div>\n",
       "</div>\n",
       "</div>\n",
       "</div>\n",
       "<script src=\"src/main.js\"></script>\n",
       "\n",
       "</body></html>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_and_parse(url):\n",
    "    response = r.get(url);\n",
    "    if response.status_code == 200:\n",
    "        return BeautifulSoup(response.content, \"html5lib\")\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "get_and_parse(\"https://procyonic.org\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above looks like text, but what BeautifulSoup gives us is a Document Object that we can search. But let's pick a more interesting target.\n",
    "\n",
    "There are varying levels of complexity associated with scraping different web pages. Let's stick with something simple: hackernews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html lang=\"en\" op=\"news\"><head><meta content=\"origin\" name=\"referrer\"/><meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/><link href=\"news.css?fVvsFovZkUlGSpAVgifJ\" rel=\"stylesheet\" type=\"text/css\"/>\\n        <link href=\"favicon.ico\" rel=\"shortcut icon\"/>\\n          <link href=\"rss\" rel=\"alternate\" title=\"RSS\" type=\"application/rss+xml\"/>\\n        <title>Hacker News</title></head><body><center><table bgcolor=\"#f6f6ef\" border=\"0\" cellpadding=\"0\" cellspacing=\"0\" id=\"hnmain\" width=\"85%\">\\n        <tbody><tr><td bgcolor=\"#ff6600\"><table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" style=\"padding:2px\" width=\"100%\"><tbody><tr><td style=\"width:18px;padding-right:4px\"><a href=\"https://news.ycombinator.com\"><img height=\"18\" src=\"y18.gif\" style=\"border:1px white solid;\" width=\"18\"/></a></td>\\n                  <td style=\"line-height:12pt; height:10px;\"><span class=\"pagetop\"><b class=\"hnname\"><a href=\"news\">Hacker News</a></b>\\n              <a href=\"newest\">new</a> | <a href=\"front\"'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(get_and_parse(\"https://news.ycombinator.com\"))[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = get_and_parse(\"https://news.ycombinator.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cursory examination of the HTML of the YCombinator Page tells us we are interested in link (anchor) tags with class \"storylink\". We can use the \"find_all\" method on the page object to get all of the links like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"news\">Hacker News</a>,\n",
       " <a href=\"newest\">new</a>,\n",
       " <a href=\"front\">past</a>,\n",
       " <a href=\"newcomments\">comments</a>,\n",
       " <a href=\"ask\">ask</a>,\n",
       " <a href=\"show\">show</a>,\n",
       " <a href=\"jobs\">jobs</a>,\n",
       " <a href=\"submit\">submit</a>,\n",
       " <a href=\"login?goto=news\">login</a>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find_all(\"a\")[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this isn't really what we want: there are too many links here. We want to restrict down to a single class of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a class=\"storylink\" href=\"https://www.reuters.com/article/idUSKBN27W2MB\">Twitter names famed hacker 'Mudge' as head of security</a>,\n",
       " <a class=\"storylink\" href=\"https://www.themvpsprint.com/p/how-and-when-to-acquire-saas-users\">My side projects always fail. This time is different.</a>,\n",
       " <a class=\"storylink\" href=\"https://stopa.io/post/269\">What Gödel Discovered</a>,\n",
       " <a class=\"storylink\" href=\"https://tomcam.github.io/postgres/\">PostgreSQL psql command line tutorial and cheat sheet</a>,\n",
       " <a class=\"storylink\" href=\"https://www.tesorio.com/careers#job-openings\" rel=\"nofollow\">Tesorio Is Hiring a Senior Product Manager and Senior Engineers</a>,\n",
       " <a class=\"storylink\" href=\"https://mullvad.net/en/blog/2020/11/16/big-no-big-sur-mullvad-disallows-apple-apps-bypass-firewall/\">Big no on Big Sur: Mullvad disallows Apple apps to bypass firewall</a>,\n",
       " <a class=\"storylink\" href=\"https://scarybeastsecurity.blogspot.com/2020/11/reverse-engineering-forgotten-1970s.html\">Reverse engineering a forgotten 1970s Intel dual core beast: 8271, a new ISA</a>,\n",
       " <a class=\"storylink\" href=\"https://www.theguardian.com/world/2020/nov/16/moderna-covid-vaccine-candidate-almost-95-effective-trials-show\">Moderna Covid vaccine candidate almost 95% effective, trials show</a>,\n",
       " <a class=\"storylink\" href=\"https://www.vice.com/en/article/wjwebw/yelp-is-sneakily-replacing-restaurants-phone-numbers-so-grubhub-can-take-a-cut\">Yelp Is Screwing over Restaurants by Quietly Replacing Their Phone Numbers</a>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page.find_all(\"a\",class_=\"storylink\")[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. Perhaps now we want to save the url and the story title to a csv file somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p derived_data\n",
    "import pandas as pd\n",
    "\n",
    "anchors = page.find_all(\"a\",class_=\"storylink\")\n",
    "\n",
    "df = pd.DataFrame([(el.get(\"href\"),\n",
    "                    el.getText()) \n",
    "                   for el in anchors], \n",
    "                  columns=[\"url\",\"headline\"])\n",
    "df.to_csv(\"derived_data/hackernews_basic_headlines.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You now know how to do basic web scraping!\n",
    "\n",
    "But there is a lot more to this activity. For instance: We might want to scrape more information about these submissions (like the submitter name and how many votes).\n",
    "\n",
    "We could search for each element of each type separately but we'd risk subtle errors in ordering. The better thing to do is iterate over an element that contains everything we want (if possible).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr class=\"athing\" id=\"25111726\">\n",
       "       <td align=\"right\" class=\"title\" valign=\"top\"><span class=\"rank\">1.</span></td>      <td class=\"votelinks\" valign=\"top\"><center><a href=\"vote?id=25111726&amp;how=up&amp;goto=news\" id=\"up_25111726\"><div class=\"votearrow\" title=\"upvote\"></div></a></center></td><td class=\"title\"><a class=\"storylink\" href=\"https://github.blog/2020-11-16-standing-up-for-developers-youtube-dl-is-back/\">YouTube-dl's repository has been restored</a><span class=\"sitebit comhead\"> (<a href=\"from?site=github.blog\"><span class=\"sitestr\">github.blog</span></a>)</span></td></tr>,\n",
       " <tr class=\"athing\" id=\"25115754\">\n",
       "       <td align=\"right\" class=\"title\" valign=\"top\"><span class=\"rank\">2.</span></td>      <td class=\"votelinks\" valign=\"top\"><center><a href=\"vote?id=25115754&amp;how=up&amp;goto=news\" id=\"up_25115754\"><div class=\"votearrow\" title=\"upvote\"></div></a></center></td><td class=\"title\"><a class=\"storylink\" href=\"https://www.reuters.com/article/idUSKBN27W2MB\">Twitter names famed hacker 'Mudge' as head of security</a><span class=\"sitebit comhead\"> (<a href=\"from?site=reuters.com\"><span class=\"sitestr\">reuters.com</span></a>)</span></td></tr>,\n",
       " <tr class=\"athing\" id=\"25114214\">\n",
       "       <td align=\"right\" class=\"title\" valign=\"top\"><span class=\"rank\">3.</span></td>      <td class=\"votelinks\" valign=\"top\"><center><a href=\"vote?id=25114214&amp;how=up&amp;goto=news\" id=\"up_25114214\"><div class=\"votearrow\" title=\"upvote\"></div></a></center></td><td class=\"title\"><a class=\"storylink\" href=\"https://www.themvpsprint.com/p/how-and-when-to-acquire-saas-users\">My side projects always fail. This time is different.</a><span class=\"sitebit comhead\"> (<a href=\"from?site=themvpsprint.com\"><span class=\"sitestr\">themvpsprint.com</span></a>)</span></td></tr>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "athings = page.find_all(\"tr\",class_=\"athing\")\n",
    "athings[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fusl\n",
      "7 hours ago\n",
      "hide\n",
      "500 comments\n"
     ]
    }
   ],
   "source": [
    "el = athings[0]\n",
    "el.find(\"a\",class_=\"storylink\").get(\"href\")\n",
    "for f in el.next_sibling.find_all(\"a\"):\n",
    "    print(f.getText())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These elements don't quite contain everything we want but let's write some code to extract what we can. Then we'll solve getting the voting information, which is actually in the next element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://github.blog/2020-11-16-standing-up-for-developers-youtube-dl-is-back/',\n",
       " \"YouTube-dl's repository has been restored\",\n",
       " 'github.blog')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data(element):\n",
    "    a = element.find(\"a\",class_=\"storylink\");\n",
    "    url = a.get(\"href\");\n",
    "    headline = a.getText();\n",
    "    ## this business with the indexing is to remove () from the sitebit\n",
    "    site = element.find(\"span\",class_=\"sitebit\").getText().strip()[1:-1];\n",
    "    return (url, headline, site)\n",
    "\n",
    "extract_data(el)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good but we can do better - they key thing is to realize that the voting data we want is always one element after the one we searched on.\n",
    "\n",
    "Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://github.blog/2020-11-16-standing-up-for-developers-youtube-dl-is-back/',\n",
       " \"YouTube-dl's repository has been restored\",\n",
       " 'github.blog',\n",
       " 1449,\n",
       " '5 hours ago')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_data(element):\n",
    "    a = element.find(\"a\",class_=\"storylink\");\n",
    "    url = a.get(\"href\");\n",
    "    headline = a.getText();\n",
    "    ## this business with the indexing is to remove () from the sitebit\n",
    "    site = element.find(\"span\",class_=\"sitebit\").getText().strip()[1:-1];\n",
    "    ns = element.next_sibling\n",
    "    score = int(ns.find(\"span\",class_=\"score\").getText().split(\" \")[0])\n",
    "    # we punt on parsing age into a number lest very old posts are in a different unit\n",
    "    age = ns.find(\"span\",class_=\"age\").getText()\n",
    "    return (url, headline, site, score, age)\n",
    "\n",
    "extract_data(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really want to get the number of comments out of this scrape too but that tag is without a class that picks it out. That means we need to search for it explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://www.tesorio.com/careers#job-openings',\n",
       " 'Tesorio Is Hiring a Senior Product Manager and Senior Engineers',\n",
       " 'tesorio.com',\n",
       " 0,\n",
       " '9 minutes ago',\n",
       " 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_comments(element):\n",
    "    anchors = element.find_all(\"a\");\n",
    "    result = None;\n",
    "    for a in anchors:\n",
    "        txt = a.getText();\n",
    "        if 'comments' in txt:\n",
    "            result = txt;\n",
    "            break;\n",
    "    if result:\n",
    "        return result;\n",
    "    else:\n",
    "        return 0;\n",
    "\n",
    "def get_score(element):\n",
    "    score = element.find(\"span\",class_=\"score\");\n",
    "    if score:\n",
    "        return score.getText().split(\" \")[0];\n",
    "    else:\n",
    "        return 0;\n",
    "    \n",
    "\n",
    "def extract_data(element):\n",
    "    a = element.find(\"a\",class_=\"storylink\");\n",
    "    # Bail out if something went wrong\n",
    "    if not a:\n",
    "        print(\"Error on element\")\n",
    "        print(element)\n",
    "        return None\n",
    "    url = a.get(\"href\");\n",
    "    headline = a.getText();\n",
    "    ## this business with the indexing is to remove () from the sitebit\n",
    "    site = element.find(\"span\",class_=\"sitebit\").getText().strip()[1:-1];\n",
    "    ns = element.next_sibling\n",
    "    score = get_score(ns);\n",
    "    # we punt on parsing age into a number lest very old posts are in a different unit\n",
    "    age = ns.find(\"span\",class_=\"age\").getText()\n",
    "    comments = find_comments(ns);\n",
    "    if comments:\n",
    "        comments = int(comments.split()[0].strip())\n",
    "    return (url, headline, site, score, age, comments)\n",
    "\n",
    "extract_data(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put it all together and dump the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([extract_data(el) for el in athings], columns=\"url headline site score age comments\".split())\n",
    "df\n",
    "df.to_csv(\"derived_data/hackernews_headlines.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive Scraping\n",
    "------------------\n",
    "\n",
    "There is already a lot we can do with this method but what if you want to scrape more than just the first page?\n",
    "\n",
    "The answer is to pull urls out of the thing you are parsing and parse those urls too. On hacker news we might want to grab the first N pages.  There are two approaches here: reverse engineer the URL or extract the appropriate href from the right anchor tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def extract_hn_data(starting_page, n):\n",
    "    if n == 0:\n",
    "        return [];\n",
    "    else:\n",
    "        page = get_and_parse(starting_page);\n",
    "        athings = page.find_all(\"tr\",class_=\"athing\");\n",
    "        scraped = [extract_data(el) for el in athings];\n",
    "        more = \"https://news.ycombinator.com/\" + page.find(\"a\",class_=\"morelink\").get(\"href\");\n",
    "        time.sleep(1)\n",
    "        scraped.extend(extract_hn_data(more, n - 1));\n",
    "        return scraped;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('https://github.blog/2020-11-16-standing-up-for-developers-youtube-dl-is-back/',\n",
       "  \"YouTube-dl's repository has been restored\",\n",
       "  'github.blog',\n",
       "  '1742',\n",
       "  '7 hours ago',\n",
       "  506),\n",
       " ('https://www.reuters.com/article/idUSKBN27W2MB',\n",
       "  \"Twitter names famed hacker 'Mudge' as head of security\",\n",
       "  'reuters.com',\n",
       "  '87',\n",
       "  '2 hours ago',\n",
       "  8),\n",
       " ('https://stopa.io/post/269',\n",
       "  'What Gödel Discovered',\n",
       "  'stopa.io',\n",
       "  '101',\n",
       "  '2 hours ago',\n",
       "  22),\n",
       " ('https://www.themvpsprint.com/p/how-and-when-to-acquire-saas-users',\n",
       "  'My side projects always fail. This time is different.',\n",
       "  'themvpsprint.com',\n",
       "  '148',\n",
       "  '4 hours ago',\n",
       "  71),\n",
       " ('https://mullvad.net/en/blog/2020/11/16/big-no-big-sur-mullvad-disallows-apple-apps-bypass-firewall/',\n",
       "  'Big no on Big Sur: Mullvad disallows Apple apps to bypass firewall',\n",
       "  'mullvad.net',\n",
       "  '102',\n",
       "  '2 hours ago',\n",
       "  23)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_hn_data(\"https://news.ycombinator.com\",3)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webscraping Law and Etiquette\n",
    "=============================\n",
    "\n",
    "It is kind of a grey area legally, despite this Towards Data Science post called [\"Web Scraping is Now Legal\"](https://medium.com/@tjwaterman99/web-scraping-is-now-legal-6bf0e5730a78). It is probably ok to scrape any page you don't need to log in for. If you do need to log in it might be considered malicious activity.\n",
    "\n",
    "Some things to consider: \n",
    "\n",
    "1. Find out of the site has an API that you can use instead. There may even be a library in R or Python to interact with the site directly.\n",
    "2. Limit the rate at which you make requests (like we did above). A nice rule of thumb is to think about how fast a human would interact with the site and stick to that range.\n",
    "3. Cache your results. I won't implement that here, but in this example would could save every request to disk the first time we make it. When we request the result again we can check the url and read from disk instead of making another HTTP request.\n",
    "\n",
    "Other Notes\n",
    "===========\n",
    "\n",
    "By its very nature web scraping is brittle. I would hesitate to build a tool which depended on scraping a web site over and over. Web sites change all the time.  It is better to pick a target dataset and collect as much as you can.\n",
    "\n",
    "You will also want to think carefully about duplicate entries.\n",
    "\n",
    "We didn't talk about duplicates in class (we should have) but they are one of the biggest ways you can screw up a data science project. Consider: if your data set contains a significant number of duplicates then your train/test split is necessarily invalid: some of the examples you showed the model during training are in your test set.\n",
    "\n",
    "Duplicates are very common in scraped datasets because pages are updated in real time. News items on the front page of Hackernews might be on page two one second later, which means we record them twice. I've also encountered near-duplicates scraping Yahoo Answers because that service appears to link to previous versions of the same question. You may need a fairly sophisticated deduplication method to detect this kind of thing. It is best to be very aggressive with duplicates when you can be.\n",
    "\n",
    "Class is Over\n",
    "=============\n",
    "\n",
    "This is our last class! \n",
    "\n",
    "By now you've all been exposed as broad a survey of data science tools and methods that I could muster up. You've all done a great job with a difficult and still nascent subject and I hope that I've given you a decent set of hand holds for your future work.\n",
    "\n",
    "Here is my advice:\n",
    "\n",
    "1. practice - get out there and explore some data\n",
    "2. learn to use AWS or Azure web services so that you can easily scale a data science project if you need to. I've taught you most of what you need to use those services without learning too much more.\n",
    "3. Don't forget to use git, even on your own projects.\n",
    "4. if you want to get programming at a deeper level, learn one or more of these languages: Scheme, J, Forth, Smalltalk\n",
    "\n",
    "Thank you so much for being my first set of students. I hope my learning curve hasn't made your learning curve too much steeper. \n",
    "\n",
    "Feel free to reach out for advice at any time:\n",
    "\n",
    "toups@email.unc.edu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
